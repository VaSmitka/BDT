{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful Python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String length is: 18\n",
      "String contains words: ['to', 'be', 'Or', 'NOT', 'to', 'be']\n",
      "String contains unique words: {'Or', 'be', 'to', 'NOT'}\n",
      "String to lowercase:  to be or not to be\n",
      "Array/list length (number of words) is: 6\n",
      "The first item in array is: to\n",
      "The last item in array is: be\n"
     ]
    }
   ],
   "source": [
    "my_string = \"to be Or NOT to be\"\n",
    "my_list = my_string.split()\n",
    "print 'String length is:', len(my_string)\n",
    "print 'String contains words:', my_string.split()\n",
    "print 'String contains unique words:', set(my_string.split())\n",
    "print 'String to lowercase: ', my_string.lower()\n",
    "print 'Array/list length (number of words) is:', len(my_list)\n",
    "print 'The first item in array is:', my_list[0]\n",
    "print 'The last item in array is:', my_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the interactive shell PySpark\n",
    "\n",
    "`pyspark --master yarn --num-executors 4`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(verse):    \n",
    "    return verse.split(' ')\n",
    "\n",
    "lines = sc.textFile(\"/user/pascepet/data/bible.txt\")\n",
    "words = lines.flatMap(split_string)\n",
    "pairs = words.map(lambda word: (word, 1))\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "counts.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust the above example \"word count\" (in any order):\n",
    "\n",
    "1. Choose RDD which is useful to be cached and cache it.\n",
    "1. Sort the results by frequency in descending order (Hint: sortBy nebo sortByKey).\n",
    "1. Count the word no matter of upper- or lowercase (god, God and GOD should be same word).\n",
    "1. Omit the identifying part of each line (Bible book and chapter:verse -- divided by tab \\t).\n",
    "1. Get rid of non-alphanumeric characters from the text ('.', ':', '-' etc. -- or at least some of them).\n",
    "\n",
    "### Data\n",
    "\n",
    "`/user/pascepet/data/bible.txt` at HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. caching:**   \n",
    "`words.cache()` or `words = lines.flatMap(lambda line: line.split(\" \")).cache()`\n",
    "\n",
    "**2. sorting results:**  \n",
    "`counts_sorted = counts.sortBy(lambda x: x[1], ascending=False)`\n",
    "\n",
    "**3. case-insensitivity -- e. g. transforming words to lowercase:**  \n",
    "`words = words.map(lambda word: word.lower())`\n",
    "\n",
    "**4. separating start of row (omitting row identification):**  \n",
    "`lines = lines.map(lambda line: line.split(\"\\t\")[1])`\n",
    "\n",
    "**5. removing of non-alphanumeric characters:**  \n",
    "* removing of a particular non-alphanum. character:  \n",
    "`words = words.map(lambda word: word.replace('.', '')`\n",
    "* removing of all non-alphanum. character:  \n",
    "`import re`  \n",
    "`words = words.map(lambda word: re.sub(r'\\W+', '', word))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Count words in each verse (1 verse = 1 line) and find the verses with the highest and lowest number of words.  \n",
    "b) Do the same calculation as in a) but count only unique words in each verse.\n",
    "\n",
    "### Data\n",
    "\n",
    "`/user/pascepet/bible.txt` na HDFS\n",
    "\n",
    "### Expected result\n",
    "\n",
    "| verse_id | word_count |  \n",
    "| -------- | ----------:|  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code contains solution for a) and b) together\n",
    "# reading RDD\n",
    "lines = sc.textFile(\"/user/pascepet/data/bible.txt\") \n",
    "\n",
    "# splitting a row to id and text\n",
    "lines2 = lines.map(lambda line: line.split(\"\\t\"))\n",
    "\n",
    "# creating a pair (row id, list of words), caching\n",
    "lines3 = lines2.map(lambda line: (line[0], line[1].split(\" \"))).cache()\n",
    "# it's possible to clean words like in Task 1: to lowercase, removing of non-alphanumeric etc.\n",
    "\n",
    "# counting words and unique words\n",
    "counts = lines3.map(lambda line: (line[0], len(line[1]))).cache()\n",
    "counts_uniq = lines3.map(lambda line: (line[0], len(set(line[1])))).cache()\n",
    "\n",
    "# sorting rows by word count, printing first element\n",
    "counts_sorted_asc = counts.sortBy(lambda x: x[1], True)\n",
    "counts_sorted_desc = counts.sortBy(lambda x: x[1], False)\n",
    "counts_sorted_asc.take(1)\n",
    "counts_sorted_desc.take(1)\n",
    "counts_uniq_sorted_asc = counts_uniq.sortBy(lambda x: x[1], True)\n",
    "counts_uniq_sorted_desc = counts_uniq.sortBy(lambda x: x[1], False)\n",
    "counts_uniq_sorted_asc.take(1)\n",
    "counts_uniq_sorted_desc.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Get the state with the highest average temperature over months 6--8. Write the results in Celsius degrees.  \n",
    "b) For each month get the state with the highest average temperature in that months.\n",
    "\n",
    "### Data\n",
    "\n",
    "1. Find the file `/home/pascepet/fel_bigdata/data/teplota-usa.zip` on the local filesystem (not at HDFS).\n",
    "1. Copy this file to your working directory on the local filesystem.\n",
    "1. Unzip the file. You will get two files CSV. Join them into one file `teplota.csv`.\n",
    "1. The file `teplota.csv` copy to HDFS to your user directory.\n",
    "\n",
    "### Data description\n",
    "\n",
    "CSV file is delimited by ','. It has headers with column names which we need to filter out.        \n",
    "The temperature is in 10 * Fahrenheit degrees.     \n",
    "Some row have no temperature measured (empty string).\n",
    "\n",
    "**Columns:** id_station, month, day, hour, temperature, flag, latitude, longitude, hight, state, name\n",
    "\n",
    "\n",
    "### Expected result\n",
    "\n",
    "In a) section\n",
    "\n",
    "| stat | avg_temperature |  \n",
    "| ---- | ---------------:|  \n",
    "\n",
    "In b) section\n",
    "\n",
    "| month | state | avg_temperature |  \n",
    "| ----- | ----- | ---------------:|  \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# reading RDD, removing (filtering out) unwanted rows\n",
    "# !!! use the path to your own user directory at HDFS\n",
    "lines = sc.textFile(\"/user/pascepet/data/teplota.csv\")\n",
    "lines2 = lines.filter(lambda line: not(re.match(r'stanice', line)))\n",
    "\n",
    "# transforming a row to the structure ((month, state), temperature)\n",
    "recs = lines2.map(lambda line: line.split(\",\"))\n",
    "# removing rows with empty temperature\n",
    "recs = recs.filter(lambda rec: rec[4]!='')\n",
    "# Fahrenheita to Celsius conversion\n",
    "recs = recs.map(lambda rec: ((int(rec[1]), rec[9]), (float(rec[4])/10-32)*5/9))\n",
    "# caching\n",
    "recs = recs.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# a) state with the highest average temperature in June--August\n",
    "# removing data of other months\n",
    "recs1 = recs.filter(lambda rec: rec[0][0] in range(6,9))\n",
    "# keeping state, omitting month, preparing pairs for aggregation\n",
    "recs1 = recs1.map(lambda rec: (rec[0][1], (1, rec[1])))\n",
    "result1 = recs1.reduceByKey(lambda s,t: (s[0]+t[0], s[1]+t[1]))  # aggregation by states - count and sum\n",
    "result1 = result1.map(lambda res: (res[0], res[1][1]/res[1][0])) # average = sum/count\n",
    "result1 = result1.sortBy(lambda res: res[1], False)\n",
    "result1.take(1)\n",
    "\n",
    "# b) for each month: state with the highest average temperature\n",
    "# calculating aggregation for each pair (month, state)\n",
    "recs2 = recs.map(lambda rec: (rec[0], (1, rec[1])))\n",
    "result2 = recs2.reduceByKey(lambda s,t: (s[0]+t[0], s[1]+t[1]))\n",
    "result2 = result2.map(lambda res: (res[0], res[1][1]/res[1][0])) # now we have the average temperature for each pair (month, state)\n",
    "result2 = result2.map(lambda res: (res[0][0], (res[0][1], res[1]))) # we use month as a key and search maximum over states \n",
    "result3 = result2.reduceByKey(lambda s,t: (t if t[1]>s[1] else s))\n",
    "result3 = result3.sortBy(lambda res: res[0], True)\n",
    "result3.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
